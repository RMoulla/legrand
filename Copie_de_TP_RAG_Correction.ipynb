{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdqaNlqa1Wfp72ggp9a7CY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/legrand/blob/main/Copie_de_TP_RAG_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TP : Retrieval Augmented Generation (RAG)**\n",
        "\n",
        "Dans ce TP, nous allons construire un **Retrieval Augmented Generation (RAG)**. L'objectif est de fournir une méthode complète pour traiter un fichier PDF, en extraire des informations pertinentes, puis les utiliser pour répondre à une requête utilisateur de manière augmentée. Ce TP est divisé en trois phases distinctes :\n",
        "\n",
        "1. **Extraction et segmentation du texte** : Vous allez apprendre à extraire du texte à partir d'un fichier PDF et à le segmenter en sections de taille définie.\n",
        "2. **Calcul des embeddings et recherche des segments similaires** : Nous utiliserons des techniques d'embeddings pour trouver les sections du texte les plus pertinentes par rapport à une requête donnée.\n",
        "3. **Génération de réponses avec un modèle de type ChatGPT** : En utilisant le contexte extrait et le modèle GPT-4, nous allons générer une réponse contextualisée à la question posée par l'utilisateur.\n",
        "\n",
        "Par ailleurs, nous allons utiliser des outils comme `pdfplumber`, pour parser les documents pdf, `sentence-transformers` pour les embeddings et l'API OpenAI pour générer des réponses augmentées par le contexte."
      ],
      "metadata": {
        "id": "-YfoXO6F0ans"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQAWnX_WdaaU",
        "outputId": "29b11a0c-1121-4187-f54e-01a13826288c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Étape 1 : Extraction et segmentation du texte**\n",
        "\n",
        "Dans cette première étape, nous allons nous concentrer sur l'extraction du texte d'un fichier PDF et sa segmentation en blocs de taille définie. L'objectif est de rendre le texte extrait plus facile à traiter dans les phases suivantes.\n",
        "\n",
        "### Détails :\n",
        "1. **Extraction du texte** : À l'aide de `pdfplumber`, vous allez extraire tout le texte d'un fichier PDF donné. Cette étape vous permettra d'obtenir une version brute du contenu du document, qui servira de base pour les autres étapes.\n",
        "   \n",
        "2. **Segmentation du texte** : Une fois le texte extrait, vous le diviserez en segments de taille fixe (par exemple, 500 caractères par segment) à l'aide de la fonction `textwrap.wrap()` de Python. Cela permet de découper le texte en morceaux plus faciles à manipuler dans la phase suivante où nous calculerons les similarités.\n",
        "\n",
        "Cette étape est cruciale, car elle permet de structurer le texte pour le rendre exploitable par la suite. La qualité de l'extraction et de la segmentation influencera directement les résultats des phases suivantes."
      ],
      "metadata": {
        "id": "ynXMWzIx1tti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import textwrap\n",
        "\n",
        "# Chemin vers le fichier PDF\n",
        "pdf_path = \"MMD.pdf\"\n",
        "\n",
        "# Extraction du texte et des tables\n",
        "full_text = \"\"\n",
        "tables = []  # Liste pour stocker les tables extraites\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        # Extraction du texte\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            full_text += page_text\n",
        "\n",
        "        # Extraction des tables\n",
        "        page_tables = page.extract_tables()\n",
        "        if page_tables:\n",
        "            tables.extend(page_tables)  # Ajouter les tables extraites à la liste\n",
        "\n",
        "# Segmenter le texte en utilisant textwrap.wrap avec une longueur maximale de caractères\n",
        "\n",
        "paragraphs = textwrap.wrap(full_text.strip(), width=500)\n",
        "\n",
        "# Affichage des segments extraits\n",
        "print(f\"Nombre de segments de texte extraits : {len(paragraphs)}\")\n",
        "for i, para in enumerate(paragraphs[:5], 1):\n",
        "    print(f\"Segment {i}: {para}\\n\")\n",
        "\n",
        "# Affichage des tables extraites\n",
        "if tables:\n",
        "    print(f\"Nombre de tables extraites : {len(tables)}\")\n",
        "    print(\"Première table extraite :\")\n",
        "    for row in tables[0]:\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"Aucune table trouvée dans le PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBUCEH13f05b",
        "outputId": "4a01a169-c8ce-4996-abaa-73e5538112a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de segments de texte extraits : 160\n",
            "Segment 1: Texte mining par la pratique Redha Moulla Paris,14-16octobre2024 RedhaMoulla Paris,14-16octobre2024 1/151Plan de la formation Introduction au NLP Techniques statistiques pour le NLP Machine learning pour le NLP Deep learning pour le NLP Transformers et LLMs Alignement des LLMs RedhaMoulla Paris,14-16octobre2024 2/151Introduction au NLP RedhaMoulla Paris,14-16octobre2024 3/1511956 : conf´erence de Dartmouth L’histoire du NLP est intimement li´ee `a celle de l’intelligence artificielle. Il figure\n",
            "\n",
            "Segment 2: mˆeme dans le programme de la conf´erence de Dartmouth, qui a fond´e l’IA. RedhaMoulla Paris,14-16octobre2024 4/151´ Evolution du NLP L’histoire du NLP peut ˆetre divis´ee en plusieurs phases cl´es, allant des approches symboliques aux r´evolutions r´ecentes avec les mod`eles de langage g´eants (LLMs). Ann´ees 1950 - 1980 : Approches symboliques Ann´ees 1990 - 2010 : Approches statistiques 2010 - 2018 : Deep Learning et r´eseaux de neuronaux ”classiques” 2018 - Pr´esent : L’`ere des Large\n",
            "\n",
            "Segment 3: Language Models (LLMs) RedhaMoulla Paris,14-16octobre2024 5/151Approches bas´ees sur la grammaire formelle Les approches bas´ees sur la grammaire formelle ´etaient tr`es utilis´ees dans les d´ebuts du traitement automatique du langage naturel (NLP) pour analyser la structure syntaxique des phrases. Grammaire formelle : Un ensemble de r`egles d´efinissant comment former des phrases grammaticalement correctes dans une langue. Ces approches ´etaient souvent utilis´ees pour le parsing (analyse\n",
            "\n",
            "Segment 4: syntaxique), c’est-`a-dire la construction d’arbres syntaxiques repr´esentant la structure d’une phrase. Le parsing peut ˆetre : - Descendant (top-down) : La phrase compl`ete est d´ecompos´ee en groupes syntaxiques successifs. - Ascendant (bottom-up) : Les mots sont progressivement regroup´es pour former la structure syntaxique compl`ete. RedhaMoulla Paris,14-16octobre2024 6/151Exemple de parsing de phrase Le parsing consiste `a analyser la structure syntaxique d’une phrase en construisant un\n",
            "\n",
            "Segment 5: arbre qui montre la relation entre les diff´erents composants de la phrase. Phrase : ”Le chat mange une souris.” Arbre syntaxique : D´etails : S : Phrase compl`ete (Sentence) NP : Groupe nominal (Noun Phrase) VP : Groupe verbal (Verb Phrase) Det : D´eterminant (Determinant) N : Nom (Noun) V : Verbe (Verb) RedhaMoulla Paris,14-16octobre2024 7/151Chatbot ELIZA ELIZA est un des premiers programmes de traitement automatique du langage naturel, d´evelopp´e par Joseph Weizenbaum en 1966. Il est bas´e\n",
            "\n",
            "Nombre de tables extraites : 152\n",
            "Première table extraite :\n",
            "['', None, None, '', None]\n",
            "['Texte mining par la pratique\\nRedha Moulla\\nParis,14-16octobre2024', None, None, None, None]\n",
            "['', 'RedhaMoulla', '', None, 'Paris,14-16octobre2024 1/151']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Étape 2 : Calcul des Embeddings et recherche des segments similaires**\n",
        "\n",
        "Dans cette deuxième étape, nous allons calculer des **embeddings** pour chaque segment de texte extrait et effectuer une recherche pour identifier les segments les plus similaires à une requête utilisateur donnée.\n",
        "\n",
        "### Détails :\n",
        "1. **Calcul des embeddings** : Nous allons utiliser la bibliothèque `sentence-transformers` pour générer des vecteurs d’embeddings pour chaque segment de texte. Ces vecteurs capturent les caractéristiques sémantiques des segments, permettant ainsi de comparer leur pertinence par rapport à une requête donnée.\n",
        "\n",
        "2. **Recherche des segments les plus similaires** : Une fois les embeddings calculés, nous allons utiliser la **similarité cosinus** pour mesurer la proximité entre l’embedding de la requête utilisateur et les embeddings des segments de texte. Les segments les plus proches seront sélectionnés pour la prochaine étape.\n",
        "\n",
        "3. **Sélection des top-N segments** : Nous sélectionnerons les `n` segments les plus similaires à la requête, qui serviront de contexte pour générer une réponse augmentée dans la phase suivante.\n",
        "\n",
        "Cette étape est essentielle pour filtrer le contenu extrait en fonction de sa pertinence par rapport à la requête utilisateur. Elle permet de s'assurer que seuls les segments les plus utiles sont utilisés pour générer une réponse cohérente et contextuelle."
      ],
      "metadata": {
        "id": "960SkcE_2ZXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J76ZLil3hkxx",
        "outputId": "e5e97753-b59d-49c2-a3f9-cf0bab2ce380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Charger un modèle pré-entraîné de sentence-transformers\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "TdWdJsmch5n7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e5ebc8-8442-4011-be7c-520a8d6acfe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des embeddings pour chaque segmnt de texte (tableaux NumPy par défaut)\n",
        "embeddings = model.encode(paragraphs)"
      ],
      "metadata": {
        "id": "vTy5F9FrtrX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Requête utilisateur pour laquelle nous cherchons les segments similaires\n",
        "query = \"Quel est le nombre de paramètres de GPT-3 ?\"\n",
        "query_embedding = model.encode(query)\n",
        "\n",
        "# Calcul des similarités cosinus entre la requête et les segments\n",
        "similarities = util.cos_sim(query_embedding, embeddings)\n",
        "\n",
        "# Sélection des top-N documents les plus similaires\n",
        "top_n = 10\n",
        "top_results = similarities.topk(top_n)[1]  # Indices des top-N documents similaires\n",
        "\n",
        "# Accéder aux indices des résultats top-N\n",
        "top_n_paragraphs = [paragraphs[i] for i in top_results[0]]\n",
        "\n",
        "# Affichage des top-N segments les plus similaires\n",
        "for i, para in enumerate(top_n_paragraphs, 1):\n",
        "    print(f\"Segment {i} similaire : {para}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNSwDPvXhieY",
        "outputId": "482edd81-0e09-441f-8c60-71732715d824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment 1 similaire : 136/151Architecture de GPT-3 GPT-3 repose sur une architecture Transformer am´elior´ee, optimis´ee pour traiter efficacement de grandes quantit´es d’informations et g´en´erer des r´eponses coh´erentes et contextuellement pertinentes. Taille du mod`ele : Avec 175 milliards de param`etres, GPT-3 est l’un des mod`eles de langue les plus grands et les plus complexes `a ce jour. M´ecanisme d’attention : L’attention multi-tˆetes permet au mod`ele de pond´erer diff´eremment les parties d’un input,\n",
            "\n",
            "Segment 2 similaire : validation crois´ee. Le nombre de variable tir´ees pour chaque noeud est g´en´eralement donn´e par √ p pour la classification et p pour la r´egression. Cet hyperparam`etre d´epend 3 cependant du probl`eme consid´er´e. Lorsque le nombre de vairable est ´elev´e alors que le nombre de variables r´eellement partinente est faible, la probabilit´e que les p variables s´electionn´ees pour chaque partitionnement incluent des variables pertinentes devient faible, et les performances du mod`ele en termes\n",
            "\n",
            "Segment 3 similaire : Transformer 3) est le mod`ele de langue d´evelopp´e par OpenAI, repr´esentant la troisi`eme g´en´eration de la s´erie GPT. Avec 175 milliards de param`etres, GPT-3 pousse les limites de la g´en´eration de texte et de la compr´ehension du langage naturel. Capacit´es g´en´erales : GPT-3 excelle dans une vari´et´e de tˆaches de TALN sans fine-tuning sp´ecifique, grˆace `a sa puissance de mod´elisation du langage. Architecture autor´egressive : Utilise un mod`ele Transformer pour pr´edire le mot\n",
            "\n",
            "Segment 4 similaire : volumineux : Comme GPT-3, ChatGPT est d’abord pr´e-entraˆın´e sur un vaste ensemble de donn´ees textuelles, englobant un large ´eventail de la litt´erature disponible sur Internet, pour apprendre une compr´ehension g´en´erale du langage. 2 Fine-tuning supervis´e : Ensuite, ChatGPT est affin´e sur des dialogues sp´ecifiques pour am´eliorer ses comp´etences conversationnelles. Cette ´etape utilise des paires question-r´eponse et des conversations pour enseigner au mod`ele des structures de\n",
            "\n",
            "Segment 5 similaire : des tˆaches sp´ecifiques avec tr`es peu d’exemples. D´efinition : Le few-shot learning d´esigne la capacit´e d’un mod`ele `a apprendre une nouvelle tˆache `a partir d’un tr`es petit nombre d’exemples d’entraˆınement, souvent seulement quelques-uns. M´ecanisme dans GPT-3 : GPT-3 utilise des prompts contenant quelques exemples de la tˆache d´esir´ee pourguiderlemod`elesurcequiestattendu,avantdepr´esenterlaquestionou la tˆache`a r´esoudre. Le mod`ele g´en´eralise ensuite`a partir de ces exemples\n",
            "\n",
            "Segment 6 similaire : Paris,14-16octobre2024 37/151Apprentissage supervis´e RedhaMoulla Paris,14-16octobre2024 38/151Formalisation du probl`eme de l’apprentissage supervis´e Comment choisir un mod`ele parmi tous les mod`eles possibles ? A priori sur la classe du mod`ele (biais inductif). Minimisation du risque empirique. RedhaMoulla Paris,14-16octobre2024 39/151Principes de l’apprentissage supervis´e La construction d’un mod`ele en apprentissage supervis´e repose sur trois piliers essentiels qui guident le processus\n",
            "\n",
            "Segment 7 similaire : jeu de validation et un jeu de test. On entraˆıne M mod`eles sur le jeu d’entraˆınement. On ´evalue les performances respectives des M mod`eles sur le jeu de validation et on s´electionne le meilleur. Le mod`ele s´electionn´e est ensuite ´evalu´e sur le jeu de test. Id´ealement, le jeu de test est ainsi utilis´e une seule fois. RedhaMoulla Paris,14-16octobre2024 45/151Validation crois´ee K-fold La validation crois´ee est une m´ethode plus robuste pour ´evaluer les performances des mod`eles. La\n",
            "\n",
            "Segment 8 similaire : moyenne harmonique de la pr´ecision et du rappel. Pre´cisin×Rappel Score F1=2 Pre´cision+Rappel RedhaMoulla Paris,14-16octobre2024 50/151Mod`eles classiques de machine learning RedhaMoulla Paris,14-16octobre2024 51/151R´egression lin´eaire simple Soit un ensemble de n obeservations x ,x ,...,x avec les labels correspondants 1 2 n y ,y ,...,y , on cherche le mod`ele lin´eaire qui ajuste le mieux ces donn´ees. 1 2 n yˆ=β +β x 0 1 RedhaMoulla Paris,14-16octobre2024 52/151Minimisation du risque\n",
            "\n",
            "Segment 9 similaire : (y|x)exp (9) Z(x) ref β ou` : π (y|x) est la politique de r´ef´erence. ref r(x,y) est la fonction de r´ecompense refl´etant les pr´ef´erences humaines. Z(x) est la fonction de partition, assurant la normalisation. β est un param`etre de temp´erature contrˆolant la contrainte de divergence KL. RedhaMoulla Paris,14-16octobre2024 148/151Mod`ele de Bradley-Terry Le mod`ele de Bradley-Terry joue un rˆole cl´e dans DPO en simplifiant la relation entre les pr´ef´erences humaines et la politique du\n",
            "\n",
            "Segment 10 similaire : g´en´eration de la s´equence de sortie. A` chaque ´etape de d´ecodage, le mod`ele calcule un score d’attention entre l’´etat cach´e du d´ecodeur et chaque ´etat cach´e de l’encodeur. Ces scores sont utilis´es pour cr´eer un vecteur de contexte pond´er´e, qui est ensuite pass´e au d´ecodeur pour g´en´erer l’´el´ement suivant de la s´equence de sortie. exp(score(h ,h¯ )) α = t j (4) tj (cid:80)Tx exp(score(h ,h¯ )) k=1 t k c =(cid:88)Tx α h¯ (5) t tj j j=1 ou` h est l’´etat cach´e du d´ecodeur, h¯\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Étape 3 : Génération de réponses avec ChatGPT**\n",
        "\n",
        "Dans cette troisième, nous allons utiliser les segments de texte sélectionnés et une requête utilisateur pour générer une réponse contextuelle en utilisant l'API OpenAI, avec un modèle de type **ChatGPT**.\n",
        "\n",
        "### Objectifs :\n",
        "- Utiliser le modèle **GPT-4** pour générer une réponse basée sur les segments de texte les plus similaires à la requête.\n",
        "- Envoyer les segments sélectionnés et la requête utilisateur sous forme de **messages** à l’API ChatGPT.\n",
        "- Obtenir une réponse contextuelle, augmentée par les informations pertinentes extraites du texte.\n",
        "\n",
        "### Détails :\n",
        "1. **Création du contexte** : Nous allons concaténer les segments sélectionnés lors de l’étape précédente afin de créer un contexte cohérent à transmettre au modèle GPT-4. Ce contexte servira à fournir un maximum d’informations pertinentes pour générer une réponse précise.\n",
        "\n",
        "2. **Appel à l’API ChatGPT** : En utilisant le contexte et la requête de l’utilisateur, nous formulerons un prompt que nous enverrons à l’API ChatGPT. Le prompt sera structuré sous forme de messages (avec les rôles \"system\" et \"user\") pour que le modèle comprenne le contexte de la conversation.\n",
        "\n",
        "3. **Génération et récupération de la réponse** : Le modèle GPT-4 générera une réponse basée sur le contexte fourni. La réponse sera ensuite récupérée et affichée comme résultat final.\n",
        "\n",
        "Cette étape finalise le processus de **RAG** en combinant la recherche d’informations pertinentes dans un corpus de texte et la génération de réponses intelligentes basées sur ces informations."
      ],
      "metadata": {
        "id": "3CNzsVht3EjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3xLVsiylPDd",
        "outputId": "fa702f34-d951-43da-a592-c27cb4b46de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "openai.api_key = \" \"\n",
        "\n",
        "# Concaténer les top-N segments en un seul contexte\n",
        "context = \"\\n\".join(top_n_paragraphs)\n",
        "prompt = f\"Contexte :\\n{context}\\n\\nQuestion : {query}\\nRéponse :\"\n",
        "\n",
        "# Appeler l'API d'OpenAI\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4o\",  # Utilisation du modèle gpt-4o\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Tu es un assistant specialisé dans la recherche d'information à partir de documents fournis. Tes réponses doivent absolument provenir du contexte fourni.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Extraire et afficher la réponse générée\n",
        "generated_response = response['choices'][0]['message']['content'].strip()\n",
        "print(f\"Réponse générée : {generated_response}\")"
      ],
      "metadata": {
        "id": "ZkSVbRgtjYIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726f6aa0-f6ed-4326-827d-413127f54479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Réponse générée : GPT-3 possède 175 milliards de paramètres.\n"
          ]
        }
      ]
    }
  ]
}